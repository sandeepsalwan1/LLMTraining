{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "quit', 'exit', or 'stop' to end.\")\n",
    "\n",
    "conversation_history = [] # Keep for context, but how it's used with processor is now key\n",
    "MAX_HISTORY_TURNS = 3\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input_text = input(f\"You (use '{IMAGE_TOKEN}' for image): \") # This is your raw query\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"\\nExiting Q&A session.\"); break\n",
    "    if user_input_text.lower() in [\"quit\", \"exit\", \"stop\"]: break\n",
    "    if not user_input_text.strip(): continue\n",
    "\n",
    "    pil_image = None\n",
    "    if IMAGE_TOKEN in user_input_text:\n",
    "        if not processor or not hasattr(processor, 'image_processor'):\n",
    "            print(f\"WARNING: '{IMAGE_TOKEN}' found, but no valid image processor loaded.\")\n",
    "        else:\n",
    "            image_path_or_url_input = input(f\"Path/URL for image (e.g., 'test1.jpg' or URL): \")\n",
    "            if image_path_or_url_input.strip():\n",
    "                image_actual_path = image_path_or_url_input\n",
    "                if not '/' in image_actual_path and not image_actual_path.startswith(\"http\"):\n",
    "                    image_actual_path = os.path.join(\"/content\", image_actual_path)\n",
    "                    print(f\"Assuming image path is: {image_actual_path}\")\n",
    "                try:\n",
    "                    if image_actual_path.startswith((\"http://\", \"https://\")):\n",
    "                        pil_image = Image.open(requests.get(image_actual_path, stream=True).raw).convert(\"RGB\")\n",
    "                    elif os.path.exists(image_actual_path):\n",
    "                        pil_image = Image.open(image_actual_path).convert(\"RGB\")\n",
    "                    else: print(f\"ERROR: Image file not found at: {image_actual_path}\")\n",
    "                    if pil_image: print(f\"Image loaded successfully: {image_actual_path}\")\n",
    "                except Exception as e: print(f\"Failed to load image '{image_actual_path}': {e}\")\n",
    "            else: print(f\"No image path provided despite '{IMAGE_TOKEN}' token.\")\n",
    "\n",
    "    # --- REVISED PROMPT PREPARATION FOR PROCESSOR - STRATEGY 3 ---\n",
    "    # For this strategy, if an image is present, we pass the *raw user text*\n",
    "    # (which contains the IMAGE_TOKEN) directly to the processor.\n",
    "    # The processor itself might then apply an internal chat template or expect\n",
    "    # the model to handle the raw input.\n",
    "    # For text-only, or if the processor doesn't seem to handle chat, we might still\n",
    "    # use the full chat template.\n",
    "\n",
    "    text_to_give_processor = user_input_text # Start with the raw user input\n",
    "\n",
    "    # If no image, or if processor seems to need full templating for text-only,\n",
    "    # then apply the chat template. Many VLMs expect the image token in raw user query.\n",
    "    # We still need to form a full prompt for generation later *if* the processor\n",
    "    # only returns partial input_ids (e.g. only for the user turn).\n",
    "\n",
    "    # Let's construct the full chat context for the final model.generate call\n",
    "    # This uses the Gemma-3 template.\n",
    "    current_chat_turn_message_list = [{\"role\": \"user\", \"content\": user_input_text}]\n",
    "    full_chat_history_for_template = conversation_history + current_chat_turn_message_list\n",
    "    full_prompt_for_generation = tokenizer.apply_chat_template(\n",
    "            full_chat_history_for_template,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "    # print(f\"DEBUG: Full prompt for model.generate (if processor only handles current turn):\\n---\\n{full_prompt_for_generation}\\n---\")\n",
    "\n",
    "\n",
    "    # The text passed to the processor for image handling is just the user's current utterance\n",
    "    # if an image is present. This is a common pattern for LLaVA-like models.\n",
    "    if pil_image and IMAGE_TOKEN in user_input_text:\n",
    "        text_for_processor_call = user_input_text\n",
    "        print(f\"DEBUG: Using raw user input for processor text (with image): '{text_for_processor_call}'\")\n",
    "    else:\n",
    "        # If no image, or if the token isn't in the user text, send the fully templated prompt\n",
    "        # This assumes the processor can handle a fully templated prompt for text-only.\n",
    "        text_for_processor_call = full_prompt_for_generation\n",
    "        print(f\"DEBUG: Using fully templated prompt for processor text (no image/token): '{text_for_processor_call}'\")\n",
    "\n",
    "\n",
    "    model_inputs = None\n",
    "    try:\n",
    "        if pil_image and IMAGE_TOKEN in text_for_processor_call and processor and hasattr(processor, 'image_processor'):\n",
    "            # Pass the simpler text_for_processor_call\n",
    "            model_inputs = processor(text=text_for_processor_call, images=pil_image, return_tensors=\"pt\").to(model.device)\n",
    "            print(f\"DEBUG: Processor created inputs WITH IMAGE (using raw user text). Keys: {list(model_inputs.keys()) if hasattr(model_inputs, 'keys') else 'N/A'}\")\n",
    "            if 'pixel_values' not in model_inputs: print(\"WARNING: 'pixel_values' MISSING from processor output with image!\")\n",
    "\n",
    "            # **IMPORTANT CHECK**: If the processor with raw text ONLY returns input_ids for the *user_input_text* part,\n",
    "            # we might need to manually prepend history token_ids for the model.generate call if the model is not\n",
    "            # inherently conversational with just the processor's output.\n",
    "            # For now, let's assume model.generate can take just the processor's output.\n",
    "            # If the processor's output `input_ids` are very short (just the user query),\n",
    "            # we might need to use `full_prompt_for_generation` tokenized and then combine image features.\n",
    "            # This gets complex and depends heavily on the processor and model architecture.\n",
    "\n",
    "        elif processor: # Text-only, pass the fully templated prompt\n",
    "            model_inputs = processor(text=text_for_processor_call, return_tensors=\"pt\").to(model.device)\n",
    "            print(f\"DEBUG: Processor created inputs (TEXT-ONLY, using templated/raw). Keys: {list(model_inputs.keys()) if hasattr(model_inputs, 'keys') else 'N/A'}\")\n",
    "        else: # Fallback\n",
    "            print(\"DEBUG: Using tokenizer directly (fallback, text-only, templated).\")\n",
    "            model_inputs = tokenizer([text_for_processor_call], return_tensors=\"pt\").to(model.device) # text_for_processor_call is full prompt here\n",
    "\n",
    "        if not model_inputs or 'input_ids' not in model_inputs:\n",
    "            print(\"ERROR: 'input_ids' not found in model_inputs after processing.\"); continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during input processing: {e}\")\n",
    "        print(f\"  Text prompt sent to processor was: '{text_for_processor_call}'\")\n",
    "        print(f\"  Image was present: {True if pil_image and IMAGE_TOKEN in text_for_processor_call else False}\")\n",
    "        continue\n",
    "\n",
    "    print(\"Model: \", end=\"\", flush=True)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 768, \"use_cache\": True,\n",
    "        \"temperature\": 0.7, \"top_p\": 0.9, \"top_k\": 50,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": actual_model_for_config.config.pad_token_id,\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # The `model_inputs` from the processor should ideally be directly passable.\n",
    "            generated_outputs = model.generate(**model_inputs, **generate_kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR during model.generate: {e}\");\n",
    "            print(f\"  Input keys passed to generate: {list(model_inputs.keys()) if hasattr(model_inputs, 'keys') else 'N/A'}\")\n",
    "            continue\n",
    "    print()\n",
    "\n",
    "    prompt_len = model_inputs['input_ids'].shape[1]\n",
    "    response_ids = generated_outputs[0][prompt_len:]\n",
    "    assistant_response_text = tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input_text}) # Store raw user input\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_text})\n",
    "    if len(conversation_history) > MAX_HISTORY_TURNS * 2:\n",
    "        conversation_history = conversation_history[-(MAX_HISTORY_TURNS * 2):]\n",
    "\n",
    "print(\"\\n--- Interactive Q&A Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
