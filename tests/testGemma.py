# -*- coding: utf-8 -*-
"""Gemma3_4B_Inference_Only.ipynb

Automatically generated by Colab.

Make sure to select a GPU runtime (e.g., T4) in Colab:
Runtime -> Change runtime type -> Hardware accelerator -> GPU (T4)
"""

# ---------------------------------------------------------------------------------
# 1. Installation
# ---------------------------------------------------------------------------------
# This cell handles Unsloth installation.
# It's designed to work correctly in Colab.
# %%capture # Use %%capture to hide pip output if desired
import os
if "COLAB_ALWAYS_INSTALL_UNSLOTH" not in os.environ: # To prevent re-install on every run, you can set this env var
    print("Installing Unsloth and dependencies for Colab...")
    !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
    !pip install --no-deps "xformers<0.0.26" trl peft accelerate bitsandbytes
    !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
    os.environ["COLAB_ALWAYS_INSTALL_UNSLOTH"] = "1" # Set it after first install
else:
    print("Unsloth and dependencies likely already installed.")

# ---------------------------------------------------------------------------------
# 2. Load Model and Tokenizer with Unsloth
# ---------------------------------------------------------------------------------
from unsloth import FastModel
import torch

# Define the model we want to load.
# For Gemma 3 4B Instruct, "unsloth/gemma-3-4b-it" is a good choice.
# Unsloth also provides pre-quantized versions like "unsloth/gemma-3-4b-it-unsloth-bnb-4bit"
# but FastModel.from_pretrained can handle the quantization on the fly.
model_name = "unsloth/gemma-3-4b-it"
max_seq_length = 2048 # You can adjust this

print(f"Loading model: {model_name}")
model, tokenizer = FastModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    load_in_4bit=True,         # Essential for running larger models on Colab T4
    load_in_8bit=False,        # Use 4-bit for less memory
    # token = "hf_...",       # Add your Hugging Face token if dealing with gated models
                               # (gemma-3-4b-it from Unsloth is not gated)
    # full_finetuning=False,  # Not relevant for inference-only setup
    #
    # Optional: Specify a specific dtype if needed, though Unsloth handles this well.
    # dtype = torch.bfloat16, # or torch.float16 for older GPUs like T4
)
print("Model and tokenizer loaded successfully.")

# ---------------------------------------------------------------------------------
# 3. (Optional but common for Unsloth workflows) Add LoRA Adapters
# Even if not fine-tuning yet, the Unsloth workflow often involves PEFT.
# For pure inference on the base model, this step isn't strictly necessary,
# but the provided notebook includes it, and it prepares the model for potential fine-tuning.
# If you only want to run the *base* model without any LoRA layers, you could skip this.
# However, for compatibility with the rest of the notebook's fine-tuning flow, let's include it.
# ---------------------------------------------------------------------------------
print("Adding PEFT (LoRA) adapters...")
model = FastModel.get_peft_model(
    model,
    r=8,  # LoRA rank (doesn't significantly impact inference if adapters are not trained)
    lora_alpha=8,
    lora_dropout=0,
    bias="none",
    # You can choose to target specific modules or use defaults:
    # target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj",],
    # Or let Unsloth pick optimal ones for Gemma:
    finetune_vision_layers     = False, # Gemma 3 4B IT is text-only
    finetune_language_layers   = True,
    finetune_attention_modules = True,
    finetune_mlp_modules       = True,
    random_state=3407,
)
print("PEFT adapters added.")

# ---------------------------------------------------------------------------------
# 4. Set up Chat Template for Gemma 3
# ---------------------------------------------------------------------------------
from unsloth.chat_templates import get_chat_template

print("Setting up Gemma 3 chat template...")
tokenizer = get_chat_template(
    tokenizer,
    chat_template="gemma-3",  # Gemma 3 requires this specific template
)
print("Chat template configured.")

# ---------------------------------------------------------------------------------
# 5. Perform Inference
# ---------------------------------------------------------------------------------
# Let's try a simple prompt.
# The Gemma 3 format expects a specific structure for conversations.

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Hello! Can you tell me a fun fact about Google Colab?",
            }
        ],
    }
]

# Apply the chat template to format the input correctly
# add_generation_prompt = True is crucial for generation.
print("\nFormatting prompt with chat template...")
prompt_text = tokenizer.apply_chat_template(
    messages,
    tokenize=False, # We want the string first to see it
    add_generation_prompt=True,
)
print("Formatted Prompt:\n", prompt_text)

# Now tokenize the formatted prompt string for the model
inputs = tokenizer([prompt_text], return_tensors="pt").to("cuda")

print("\nGenerating response (simple)...")
# Perform generation
outputs = model.generate(
    **inputs,
    max_new_tokens=128,        # How many new tokens to generate
    use_cache=True,            # Speeds up generation
    # Recommended Gemma-3 inference settings from the docs/notebook:
    temperature=1.0,
    top_p=0.95,
    top_k=64,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id, # Important for some models
)

# Decode the generated tokens
decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=False) # Set to False to see special tokens if needed
print("\n--- Model Response (with prompt) ---")
print(decoded_output[0])

# To get only the generated part:
generated_text_only = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
print("\n--- Model Response (generated part only) ---")
print(generated_text_only)


# ---------------------------------------------------------------------------------
# 6. Inference with TextStreamer (for token-by-token output)
# ---------------------------------------------------------------------------------
from transformers import TextStreamer

messages_stream = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Why is the sky blue during the day?",
            }
        ],
    }
]

prompt_text_stream = tokenizer.apply_chat_template(
    messages_stream,
    tokenize=False,
    add_generation_prompt=True,
)
inputs_stream = tokenizer([prompt_text_stream], return_tensors="pt").to("cuda")

streamer = TextStreamer(tokenizer, skip_prompt=True)

print(f"\nGenerating response with TextStreamer (prompt: {messages_stream[0]['content'][0]['text']})...")
print("Model says: ")
_ = model.generate(
    **inputs_stream,
    streamer=streamer,
    max_new_tokens=128,
    use_cache=True,
    # Recommended Gemma-3 inference settings:
    temperature=1.0,
    top_p=0.95,
    top_k=64,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,
)
print("\nStreaming finished.")

# ---------------------------------------------------------------------------------
# 7. Check GPU Memory
# ---------------------------------------------------------------------------------
if torch.cuda.is_available():
    gpu_stats = torch.cuda.get_device_properties(0)
    max_memory_gb = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
    reserved_memory_gb = round(torch.cuda.memory_reserved(0) / 1024 / 1024 / 1024, 3)
    allocated_memory_gb = round(torch.cuda.memory_allocated(0) / 1024 / 1024 / 1024, 3)

    print(f"\n--- GPU Memory Stats ---")
    print(f"GPU: {gpu_stats.name}")
    print(f"Total Memory: {max_memory_gb} GB")
    print(f"Reserved Memory: {reserved_memory_gb} GB")
    print(f"Allocated Memory: {allocated_memory_gb} GB")
else:
    print("CUDA is not available. Running on CPU (this will be very slow for generation).")

print("\nSetup test complete! You can now proceed to finetuning if desired.")
